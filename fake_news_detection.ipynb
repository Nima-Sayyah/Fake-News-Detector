{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71589633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445cc3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/NS/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/NS/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba145b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "news_d = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78cb25e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of News data: (20800, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of News data:\", news_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a669dde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News data columns Index(['id', 'title', 'author', 'text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"News data columns\", news_d.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b4104a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by using df.head(), we can immediately familiarize ourselves with the dataset. \n",
    "news_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d848a72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20761.000000\n",
       "mean       760.308126\n",
       "std        869.525988\n",
       "min          0.000000\n",
       "25%        269.000000\n",
       "50%        556.000000\n",
       "75%       1052.000000\n",
       "max      24234.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text Word startistics: min.mean, max and interquartile range\n",
    "txt_length = news_d.text.str.split().str.len()\n",
    "txt_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ec741a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20242.000000\n",
       "mean        12.420709\n",
       "std          4.098735\n",
       "min          1.000000\n",
       "25%         10.000000\n",
       "50%         13.000000\n",
       "75%         15.000000\n",
       "max         72.000000\n",
       "Name: title, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Title statistics \n",
    "title_length = news_d.title.str.split().str.len()\n",
    "title_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1fea122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Unreliable\n",
      "0: Reliable\n",
      "Distribution of labels:\n",
      "1    10413\n",
      "0    10387\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEGCAYAAABPdROvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQwklEQVR4nO3df+xddX3H8efLVgF1dXR8YbVltm6NGzCNtmGoiZljCd0vywyYmjEaR9KFsanLfgT2x1i2dNFMt4kTkkaxRY2sQTe6JehInRo3AvuiTCiV0IiDjkq//piiiWjxvT/up3ppvy2X76ffe/v1+3wkJ/ec9zmfc9+nab6vnHPuPTdVhSRJc/WsSTcgSVrYDBJJUheDRJLUxSCRJHUxSCRJXZZOuoFxO+OMM2r16tWTbkOSFpS77777K1U1Ndu6RRckq1evZnp6etJtSNKCkuR/jrXOS1uSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLovum+0nwro/uWnSLegkdPffXD7pFnj4L39+0i3oJPRTf37vvO7fMxJJUheDRJLUxSCRJHUxSCRJXeYtSJLcmORgkvuGasuT3J7kwfZ6+tC6a5LsS/JAkouG6uuS3NvWXZckrX5Kkn9s9TuTrJ6vY5EkHdt8npFsBzYcUbsa2F1Va4HdbZkk5wCbgHPbmOuTLGljbgC2AGvbdHifVwBfr6qfAf4OePu8HYkk6ZjmLUiq6tPA144obwR2tPkdwMVD9Zur6omqegjYB5yfZAWwrKruqKoCbjpizOF93QJcePhsRZI0PuO+R3JWVR0AaK9ntvpK4JGh7fa32so2f2T9KWOq6hDwDeAnZnvTJFuSTCeZnpmZOUGHIkmCk+dm+2xnEnWc+vHGHF2s2lZV66tq/dTUrD85LEmao3EHyWPtchXt9WCr7wfOHtpuFfBoq6+apf6UMUmWAi/g6EtpkqR5Nu4g2QVsbvObgVuH6pvaJ7HWMLipfle7/PV4kgva/Y/LjxhzeF+XAJ9o91EkSWM0b8/aSvJh4BeBM5LsB64F3gbsTHIF8DBwKUBV7UmyE7gfOARcVVVPtl1dyeATYKcBt7UJ4H3AB5LsY3Amsmm+jkWSdGzzFiRV9cZjrLrwGNtvBbbOUp8Gzpul/h1aEEmSJudkudkuSVqgDBJJUheDRJLUxSCRJHUxSCRJXQwSSVIXg0SS1MUgkSR1MUgkSV0MEklSF4NEktTFIJEkdTFIJEldDBJJUheDRJLUxSCRJHUxSCRJXQwSSVIXg0SS1MUgkSR1MUgkSV0MEklSF4NEktTFIJEkdTFIJEldDBJJUheDRJLUxSCRJHUxSCRJXQwSSVIXg0SS1GUiQZLkD5PsSXJfkg8nOTXJ8iS3J3mwvZ4+tP01SfYleSDJRUP1dUnubeuuS5JJHI8kLWZjD5IkK4E3A+ur6jxgCbAJuBrYXVVrgd1tmSTntPXnAhuA65Msabu7AdgCrG3ThjEeiiSJyV3aWgqclmQp8FzgUWAjsKOt3wFc3OY3AjdX1RNV9RCwDzg/yQpgWVXdUVUF3DQ0RpI0JmMPkqr6X+AdwMPAAeAbVfVvwFlVdaBtcwA4sw1ZCTwytIv9rbayzR9ZP0qSLUmmk0zPzMycyMORpEVvEpe2TmdwlrEGeCHwvCSXHW/ILLU6Tv3oYtW2qlpfVeunpqaeacuSpOOYxKWtXwYeqqqZqvoe8FHgVcBj7XIV7fVg234/cPbQ+FUMLoXtb/NH1iVJYzSJIHkYuCDJc9unrC4E9gK7gM1tm83ArW1+F7ApySlJ1jC4qX5Xu/z1eJIL2n4uHxojSRqTpeN+w6q6M8ktwGeBQ8DngG3A84GdSa5gEDaXtu33JNkJ3N+2v6qqnmy7uxLYDpwG3NYmSdIYjT1IAKrqWuDaI8pPMDg7mW37rcDWWerTwHknvEFJ0sj8ZrskqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6jKRIEny40luSfKFJHuTvDLJ8iS3J3mwvZ4+tP01SfYleSDJRUP1dUnubeuuS5JJHI8kLWaTOiN5F/CxqvpZ4GXAXuBqYHdVrQV2t2WSnANsAs4FNgDXJ1nS9nMDsAVY26YN4zwISdIEgiTJMuA1wPsAquq7VfV/wEZgR9tsB3Bxm98I3FxVT1TVQ8A+4PwkK4BlVXVHVRVw09AYSdKYjBQkSXaPUhvRi4EZ4P1JPpfkvUmeB5xVVQcA2uuZbfuVwCND4/e32so2f2R9tv63JJlOMj0zMzPHtiVJszlukCQ5Ncly4Iwkp7f7GMuTrAZeOMf3XAq8Arihql4OfJt2GetYbcxSq+PUjy5Wbauq9VW1fmpq6pn2K0k6jqVPs/53gbcyCI27+eEf728C75nje+4H9lfVnW35FgZB8liSFVV1oF22Oji0/dlD41cBj7b6qlnqkqQxOu4ZSVW9q6rWAH9cVS+uqjVtellV/cNc3rCqvgw8kuQlrXQhcD+wC9jcapuBW9v8LmBTklOSrGFwU/2udvnr8SQXtE9rXT40RpI0Jk93RgJAVb07yauA1cNjquqmOb7vHwAfSvIc4IvAmxiE2s4kVwAPA5e299iTZCeDsDkEXFVVT7b9XAlsB04DbmuTJGmMRgqSJB8Afhq4Bzj8R/zwJ6Wesaq6B1g/y6oLj7H9VmDrLPVp4Ly59CBJOjFGChIGf/TPaR+zlSTpB0b9Hsl9wE/OZyOSpIVp1DOSM4D7k9wFPHG4WFWvm5euJEkLxqhB8hfz2YQkaeEa9VNbn5rvRiRJC9Oon9p6nB9+a/w5wLOBb1fVsvlqTJK0MIx6RvJjw8tJLgbOn4+GJEkLy5ye/ltV/wz80oltRZK0EI16aev1Q4vPYvC9Er9TIkka+VNbvzE0fwj4EoPfCZEkLXKj3iN503w3IklamEb9YatVSf4pycEkjyX5SJJVTz9SkvSjbtSb7e9n8Dj3FzL4FcJ/aTVJ0iI3apBMVdX7q+pQm7YD/tSgJGnkIPlKksuSLGnTZcBX57MxSdLCMGqQ/A7wBuDLwAHgEgY/RiVJWuRG/fjvXwGbq+rrAEmWA+9gEDCSpEVs1DOSlx4OEYCq+hrw8vlpSZK0kIwaJM9KcvrhhXZGMurZjCTpR9ioYfBO4D+T3MLg0ShvYJbfUJckLT6jfrP9piTTDB7UGOD1VXX/vHYmSVoQRr481YLD8JAkPcWcHiMvSdJhBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4TC5L2A1mfS/KvbXl5ktuTPNhehx8SeU2SfUkeSHLRUH1dknvbuuuSZBLHIkmL2STPSN4C7B1avhrYXVVrgd1tmSTnAJuAc4ENwPVJlrQxNwBbgLVt2jCe1iVJh00kSJKsAn4NeO9QeSOwo83vAC4eqt9cVU9U1UPAPuD8JCuAZVV1R1UVcNPQGEnSmEzqjOTvgT8Fvj9UO6uqDgC01zNbfSXwyNB2+1ttZZs/sn6UJFuSTCeZnpmZOSEHIEkaGHuQJPl14GBV3T3qkFlqdZz60cWqbVW1vqrWT01Njfi2kqRRTOJXDl8NvC7JrwKnAsuSfBB4LMmKqjrQLlsdbNvvB84eGr8KeLTVV81SlySN0djPSKrqmqpaVVWrGdxE/0RVXQbsAja3zTYDt7b5XcCmJKckWcPgpvpd7fLX40kuaJ/WunxojCRpTE6m311/G7AzyRXAw8ClAFW1J8lOBj+qdQi4qqqebGOuBLYDpwG3tUmSNEYTDZKq+iTwyTb/VeDCY2y3lVl+I76qpoHz5q9DSdLT8ZvtkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6jL2IElydpJ/T7I3yZ4kb2n15UluT/Jgez19aMw1SfYleSDJRUP1dUnubeuuS5JxH48kLXaTOCM5BPxRVf0ccAFwVZJzgKuB3VW1FtjdlmnrNgHnAhuA65Msafu6AdgCrG3ThnEeiCRpAkFSVQeq6rNt/nFgL7AS2AjsaJvtAC5u8xuBm6vqiap6CNgHnJ9kBbCsqu6oqgJuGhojSRqTid4jSbIaeDlwJ3BWVR2AQdgAZ7bNVgKPDA3b32or2/yR9dneZ0uS6STTMzMzJ/QYJGmxm1iQJHk+8BHgrVX1zeNtOkutjlM/uli1rarWV9X6qampZ96sJOmYJhIkSZ7NIEQ+VFUfbeXH2uUq2uvBVt8PnD00fBXwaKuvmqUuSRqjSXxqK8D7gL1V9bdDq3YBm9v8ZuDWofqmJKckWcPgpvpd7fLX40kuaPu8fGiMJGlMlk7gPV8N/DZwb5J7Wu3PgLcBO5NcATwMXApQVXuS7ATuZ/CJr6uq6sk27kpgO3AacFubJEljNPYgqarPMPv9DYALjzFmK7B1lvo0cN6J606S9Ez5zXZJUheDRJLUxSCRJHUxSCRJXQwSSVIXg0SS1MUgkSR1MUgkSV0MEklSF4NEktTFIJEkdTFIJEldDBJJUheDRJLUxSCRJHUxSCRJXQwSSVIXg0SS1MUgkSR1MUgkSV0MEklSF4NEktTFIJEkdTFIJEldDBJJUheDRJLUxSCRJHUxSCRJXQwSSVIXg0SS1MUgkSR1WfBBkmRDkgeS7Ety9aT7kaTFZkEHSZIlwHuAXwHOAd6Y5JzJdiVJi8uCDhLgfGBfVX2xqr4L3AxsnHBPkrSoLJ10A51WAo8MLe8HfuHIjZJsAba0xW8leWAMvS0WZwBfmXQTJ4O8Y/OkW9BT+X/zsGtzIvbyomOtWOhBMtu/Th1VqNoGbJv/dhafJNNVtX7SfUhH8v/m+Cz0S1v7gbOHllcBj06oF0lalBZ6kPwXsDbJmiTPATYBuybckyQtKgv60lZVHUry+8DHgSXAjVW1Z8JtLTZeMtTJyv+bY5Kqo24pSJI0soV+aUuSNGEGiSSpi0GiOfHRNDpZJbkxycEk9026l8XCINEz5qNpdJLbDmyYdBOLiUGiufDRNDppVdWnga9Nuo/FxCDRXMz2aJqVE+pF0oQZJJqLkR5NI2lxMEg0Fz6aRtIPGCSaCx9NI+kHDBI9Y1V1CDj8aJq9wE4fTaOTRZIPA3cAL0myP8kVk+7pR52PSJEkdfGMRJLUxSCRJHUxSCRJXQwSSVIXg0SS1MUgkeZRkm89zfrVz/QptUm2J7mkrzPpxDFIJEldDBJpDJI8P8nuJJ9Ncm+S4aclL02yI8nnk9yS5LltzLokn0pyd5KPJ1kxofal4zJIpPH4DvCbVfUK4LXAO5McfvjlS4BtVfVS4JvA7yV5NvBu4JKqWgfcCGydQN/S01o66QakRSLAXyd5DfB9Bo/dP6ute6Sq/qPNfxB4M/Ax4Dzg9pY3S4ADY+1YGpFBIo3HbwFTwLqq+l6SLwGntnVHPqeoGATPnqp65fhalObGS1vSeLwAONhC5LXAi4bW/VSSw4HxRuAzwAPA1OF6kmcnOXesHUsjMkik8fgQsD7JNIOzky8MrdsLbE7yeWA5cEP7CeNLgLcn+W/gHuBV421ZGo1P/5UkdfGMRJLUxSCRJHUxSCRJXQwSSVIXg0SS1MUgkSR1MUgkSV3+H1yoO0DygZCIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=\"label\", data=news_d);\n",
    "print(\"1: Unreliable\")\n",
    "print(\"0: Reliable\")\n",
    "print(\"Distribution of labels:\")\n",
    "print(news_d.label.value_counts());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf7b9fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-aed2aa4e39e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'news_d' is not defined"
     ]
    }
   ],
   "source": [
    "print(round(news_d.label.value_counts(normalize=True),2)*100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49eeed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants that are used to sanitize the datasets \n",
    "column_n = ['id', 'title', 'author', 'text', 'label']\n",
    "remove_c = ['id','author']\n",
    "categorical_features = []\n",
    "target_col = ['label']\n",
    "text_f = ['title', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9a11705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Datasets\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd0c2c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67538a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed unused clumns\n",
    "def remove_unused_c(df,column_n=remove_c):\n",
    "    df = df.drop(column_n,axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e8c7f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute null values with None\n",
    "def null_process(feature_df):\n",
    "    for col in text_f:\n",
    "        feature_df.loc[feature_df[col].isnull(), col] = \"None\"\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db6790ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    # remove unused column\n",
    "    df = remove_unused_c(df)\n",
    "    #impute null values\n",
    "    df = null_process(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f610c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning text from unused characters\n",
    "def clean_text(text):\n",
    "    text = str(text).replace(r'http[\\w:/\\.]+', ' ')  # removing urls\n",
    "    text = str(text).replace(r'[^\\.\\w\\s]', ' ')  # remove everything but characters and punctuation\n",
    "    text = str(text).replace('[^a-zA-Z]', ' ')\n",
    "    text = str(text).replace(r'\\s\\s+', ' ')\n",
    "    text = text.lower().strip()\n",
    "    #text = ' '.join(text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f55f9635",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nltk Preprocessing include: Stop words, Stemming and Lemmetization, we use only Stop word removal\n",
    "def nltk_preprocess(text):\n",
    "    text = clean_text(text)\n",
    "    wordlist = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    #text = ' '.join([word for word in wordlist if word not in stopwords_dict])\n",
    "    #text = [ps.stem(word) for word in wordlist if not word in stopwords_dict]\n",
    "    text = ' '.join([wnl.lemmatize(word) for word in wordlist if word not in stopwords_dict])\n",
    "    return  text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ded75b7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3a58265ed751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Perform data cleaning on train and test dataset by calling clean_dataset function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# apply preprocessing on text through apply method by calling the function nltk_preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk_preprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# apply preprocessing on title through apply method by calling the function nltk_preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'news_d' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform data cleaning on train and test dataset by calling clean_dataset function\n",
    "df = clean_dataset(news_d)\n",
    "\n",
    "# apply preprocessing on text through apply method by calling the function nltk_preprocess\n",
    "df[\"text\"] = df.text.apply(nltk_preprocess)\n",
    "\n",
    "# apply preprocessing on title through apply method by calling the function nltk_preprocess\n",
    "df[\"title\"] = df.title.apply(nltk_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset after cleaning and preprocessing step\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326e8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e15edf8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-05054581411b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'black'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# generate the word cloud by passing the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtext_cloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# plotting the word cloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize the word cloud\n",
    "wordcloud = WordCloud( background_color='black', width=800, height=600)\n",
    "\n",
    "# generate the word cloud by passing the corpus\n",
    "text_cloud = wordcloud.generate(' '.join(df['text']))\n",
    "\n",
    "# plotting the word cloud\n",
    "plt.figure(figsize=(20,30))\n",
    "plt.imshow(text_cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e3bc830",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8e67dee1d20a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrue_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "true_n = ' '.join(df[df['label']==0]['text']) \n",
    "wc = wordcloud.generate(true_n)\n",
    "plt.figure(figsize=(20,30))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98944f66",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ce6d201f14df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfake_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "fake_n = ' '.join(df[df['label']==1]['text'])\n",
    "wc= wordcloud.generate(fake_n)\n",
    "plt.figure(figsize=(20,30))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dc27512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_ngrams(corpus, title, ylabel, xlabel=\"Number of Occurences\", n=2):\n",
    "    \n",
    "    \"\"\"Utility function to plot top n-grams\"\"\"\n",
    "    \n",
    "    true_b = (pd.Series(nltk.ngrams(corpus.split(), n)).value_counts())[:20]\n",
    "    true_b.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7e3afb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ee554f6dec9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_top_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Top 20 Frequently Occuring True news Bigrams'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Bigram\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'true_n' is not defined"
     ]
    }
   ],
   "source": [
    "plot_top_ngrams(true_n, 'Top 20 Frequently Occuring True news Bigrams', \"Bigram\", n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8672b526",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fake_n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-69cc2406f8bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_top_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Top 20 Frequently Occuring Fake news Bigrams'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Bigram\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fake_n' is not defined"
     ]
    }
   ],
   "source": [
    "plot_top_ngrams(fake_n, 'Top 20 Frequently Occuring Fake news Bigrams', \"Bigram\", n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac066873",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_ngrams(true_n, 'Top 20 Frequently Occuring True news Trigrams', \"Trigrams\", n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_ngrams(fake_n, 'Top 20 Frequently Occuring Fake news Trigrams', \"Trigrams\", n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers (for pipenv)\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    "\n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if is_torch_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "        if is_tf_available():\n",
    "        import tensorflow as tf\n",
    "\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model to train, base uncased BERT, text classification models \n",
    "# here: https://huggingface.co/models?filter=text-classification\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# max sequence length for each document/sentence sample\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dbb6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "\n",
    "news_df = news_d[news_d['text'].notna()]\n",
    "news_df = news_df[news_df[\"author\"].notna()]\n",
    "news_df = news_df[news_df[\"title\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, test_size=0.2, include_title=True, include_author=True):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "    text = df[\"text\"].iloc[i]\n",
    "    label = df[\"label\"].iloc[i]\n",
    "    \n",
    "    if include_title:\n",
    "        text = df[\"title\"].iloc[i] + \" - \" + text\n",
    "    \n",
    "    if include_author:\n",
    "        text = df[\"author\"].iloc[i] + \" : \" + text\n",
    "    \n",
    "    if text and label in [0, 1]:\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return train_test_split(texts, labels, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d8c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, valid_texts, train_labels, valid_labels = prepare_data(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d468efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_texts), len(train_labels))\n",
    "print(len(valid_texts), len(valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07b52f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset, truncate when passed `max_length`, pad with 0's when < `max_length`\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
    "valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547003d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsGroupsDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# convert our tokenized data into a torch Dataset\n",
    "train_dataset = NewsGroupsDataset(train_encodings, train_labels)\n",
    "valid_dataset = NewsGroupsDataset(valid_encodings, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47806379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9124e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # calculate accuracy using sklearn's function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        \n",
    "        'accuracy': acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=10,  # batch size per device during training\n",
    "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
    "    \n",
    "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "    \n",
    "    logging_steps=200,               # log & save weights each logging_steps\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73992b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    \n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=valid_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2267611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the current model after training\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the fine tuned model & tokenizer\n",
    "model_path = \"fake-news-bert-base-uncased\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81caea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(text, convert_to_label=False):\n",
    "    \n",
    "    # prepare our text into tokenized sequence\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # perform inference to our model\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # get output probabilities by doing softmax\n",
    "    probs = outputs[0].softmax(1)\n",
    "    \n",
    "    # executing argmax function to get the candidate label\n",
    "    d = {\n",
    "        0: \"reliable\",\n",
    "        1: \"fake\"\n",
    "    }\n",
    "    if convert_to_label:\n",
    "        return d[int(probs.argmax())]\n",
    "    \n",
    "    else:\n",
    "        return int(probs.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831c73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news = \"\"\"\n",
    "    Tim Tebow Will Attempt Another Comeback, This Time in Baseball - The New York Times\",Daniel Victor,\"If at first you don’t succeed, try a different sport. Tim Tebow, who was a Heisman   quarterback at the University of Florida but was unable to hold an N. F. L. job, is pursuing a career in Major League Baseball. <SNIPPED>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c67e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction(real_news, convert_to_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6448c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the test set\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# make a copy of the testing set\n",
    "new_df = test_df.copy()\n",
    "\n",
    "# add a new column that contains the author, title and article content\n",
    "new_df[\"new_text\"] = new_df[\"author\"].astype(str) + \" : \" + new_df[\"title\"].astype(str) + \" - \" + new_df[\"text\"]\n",
    "\n",
    "# get the prediction of all the test set\n",
    "new_df[\"label\"] = new_df[\"new_text\"].apply(get_prediction)\n",
    "\n",
    "# make the submission file\n",
    "final_df = new_df[[\"id\", \"label\"]]\n",
    "final_df.to_csv(\"submit_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
